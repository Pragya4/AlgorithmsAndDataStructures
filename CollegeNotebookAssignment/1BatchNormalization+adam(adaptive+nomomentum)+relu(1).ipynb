{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1BatchNormalization+adam(adaptive+nomomentum)+relu.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"c_3RVFx9BbUj","colab_type":"text"},"cell_type":"markdown","source":["# Handle the CIFAR-10 dataset\n","\n","https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/"]},{"metadata":{"id":"wYHgvYFEBbUo","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import pickle\n","import numpy as np\n","from os import listdir\n","from os.path import isfile, join\n","import os\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lwqdkPouBbUw","colab_type":"text"},"cell_type":"markdown","source":["## Create a function that will extract the data from the dataset\n","You can find the dataset here: https://www.cs.utoronto.ca/~kriz/cifar-10-python.tar.gz\n","\n","When you download the dataset you will get a bunch of pickled files <br>\n","Inside `cifar-10-batches-py`folder you will find <br>\n","1. data_batch_x files <br>\n","2. test_batch \n","\n","The data_batch_x files are the batched training data splited in x parts (in our case in 5 parts of 10000 samples each). The test_batch file contains 10000 samples for testing.\n","\n","### The stracture\n","All files has the same structure: <br>\n","1. data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n","2. labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n","\n","There is also another file called batches.meta where it describes the classes"]},{"metadata":{"id":"dE6wyiEWBbUx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Function to unpickle the dataset\n","def unpickle_all_data(directory):\n","    \n","    # Initialize the variables\n","    train = dict()\n","    test = dict()\n","    train_x = []\n","    train_y = []\n","    test_x = []\n","    test_y = []\n","    \n","    # Iterate through all files that we want, train and test\n","    # Train is separated into batches\n","    for filename in listdir(directory):\n","        if isfile(join(directory, filename)):\n","            \n","            # The train data\n","            if 'data_batch' in filename:\n","                print('Handing file: %s' % filename)\n","                \n","                # Opent the file\n","                with open(directory + '/' + filename, 'rb') as fo:\n","                    data = pickle.load(fo)\n","\n","                if 'data' not in train:\n","                    train['data'] = data[b'data']\n","                    train['labels'] = np.array(data[b'labels'])\n","                else:\n","                    train['data'] = np.concatenate((train['data'], data[b'data']))\n","                    train['labels'] = np.concatenate((train['labels'], data[b'labels']))\n","            # The test data\n","            elif 'test_batch' in filename:\n","                print('Handing file: %s' % filename)\n","                \n","                # Open the file\n","                with open(directory + '/' + filename, 'rb') as fo:\n","                    data = pickle.load(fo)\n","                \n","                test['data'] = data[b'data']\n","                test['labels'] = data[b'labels']\n","    \n","    # Manipulate the data to the propper format\n","    for image in train['data']:\n","        train_x.append(np.transpose(np.reshape(image,(3, 32,32)), (1,2,0)))\n","    train_y = [label for label in train['labels']]\n","    \n","    for image in test['data']:\n","        test_x.append(np.transpose(np.reshape(image,(3, 32,32)), (1,2,0)))\n","    test_y = [label for label in test['labels']]\n","    \n","    # Transform the data to np array format\n","    train_x = np.array(train_x)\n","    train_y = np.array(train_y)\n","    test_x = np.array(test_x)\n","    test_y = np.array(test_y)\n","    \n","    return (train_x, train_y), (test_x, test_y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v2dX7eLDBbU1","colab_type":"text"},"cell_type":"markdown","source":["## Using the function\n","\n","Returns four variables\n","1. x_train := A 3D numpy array with the images in a format (32, 32, 3) a 32x32 image with 3 dimmesions for colors RGB\n","2. y_train := A 1D numpy with the numbered labels for each sample stom x_train\n","3. x_test := same as x_train\n","4. y_test := same as y_train"]},{"metadata":{"id":"Uo79OKVoBbU2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"d3286dea-26f5-43ba-fb79-84849dc10a3b","executionInfo":{"status":"ok","timestamp":1537280675104,"user_tz":-330,"elapsed":76426,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["#/home/pragya/Desktop/IIITBangalore/Semester3/AVR/Tensorflow_AVR/sagemaker-deep-learning-master/cifar-10-keras-mxnet/sagemaker\n","from keras.datasets import cifar10 \n","#(x_train, y_train), (x_test, y_test) = unpickle_all_data(os.getcwd() + '/cifar-10-batches-py/')\n","(x_train, y_train), (x_test, y_test)=cifar10.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 60s 0us/step\n","170508288/170498071 [==============================] - 60s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"apzhNzR9BbU9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"8400cde2-62b7-45fe-d441-4322ce7e66bb","executionInfo":{"status":"ok","timestamp":1537280676647,"user_tz":-330,"elapsed":1473,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3)\n","(50000, 1)\n","(10000, 32, 32, 3)\n","(10000, 1)\n"],"name":"stdout"}]},{"metadata":{"id":"OQadpBLGBbVB","colab_type":"text"},"cell_type":"markdown","source":["## Preprocess the data\n","1. The data must be in a float32 represenation\n","2. The data should be normalized, here I chose [0,1] normalization\n","3. The labels are one-hot encoded"]},{"metadata":{"id":"emRbiM9UBbVB","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.utils import np_utils\n","\n","# Transofrm them to a float32 type\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# Normalize the input \n","x_train /= 255\n","x_test /= 255\n","\n","# One-hot Encoding\n","num_classes = 10\n","y_train = np_utils.to_categorical(y_train, num_classes)\n","y_test = np_utils.to_categorical(y_test, num_classes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PJyaoh74BbVF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"56b39c34-61fb-43a4-f6b1-06995627d74d","executionInfo":{"status":"ok","timestamp":1537280680130,"user_tz":-330,"elapsed":1630,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3)\n","(50000, 10)\n","(10000, 32, 32, 3)\n","(10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"IDDwE2__BbVG","colab_type":"text"},"cell_type":"markdown","source":["## Building the model\n","1. The input data for the model is each image sample 32x32x3\n","2. Convolve the image into 3x3 squares with padding same (https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t) and Pooling with 2x2. Add a dropout of 0.2 to avoid early overfitting\n","3. Same situation for the next two layers for 64 input size, and 128 input size\n","4. Finally, add a Dense(fully connected) layer for the output\n","\n","You can find more information about the model here: https://appliedmachinelearning.wordpress.com/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n","\n","You can try different layouts to improve the accuracy"]},{"metadata":{"id":"IzVKiAbPBbVH","colab_type":"code","colab":{}},"cell_type":"code","source":["import keras\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.datasets import cifar10\n","from keras import regularizers\n","from keras.callbacks import LearningRateScheduler\n","import numpy as np\n","\n","# Create the model\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same',\n","                 input_shape=x_train.shape[1:]))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(32, (3, 3)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","#model.add(Conv2D(64, (3, 3), padding='same'))\n","#model.add(BatchNormalization())\n","#model.add(Activation('relu'))\n","#model.add(Conv2D(64, (3, 3)))\n","#model.add(BatchNormalization())\n","#model.add(Activation('relu'))\n","#model.add(MaxPooling2D(pool_size=(2, 2)))\n","#model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(512))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","#model.add(Dense(num_classes, activation='softmax'))\n","\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2lluao8xBbVK","colab_type":"text"},"cell_type":"markdown","source":["## Augment the data\n","From one sample, extract multiple versions\n","\n","For example, rotate the image, shift, flip etc"]},{"metadata":{"id":"GTGbNLBOBbVK","colab_type":"code","colab":{}},"cell_type":"code","source":["# data augmentation\n","datagen = ImageDataGenerator(\n","    featurewise_center=False,  # set input mean to 0 over the dataset\n","    samplewise_center=False,  # set each sample mean to 0\n","    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","    samplewise_std_normalization=False,  # divide each input by its std\n","    zca_whitening=False,  # apply ZCA whitening\n","    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n","    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","    horizontal_flip=True,  # randomly flip images\n","    vertical_flip=False)  # randomly flip images\n","\n","# Compute quantities required for feature-wise normalization\n","# (std, mean, and principal components if ZCA whitening is applied).\n","datagen.fit(x_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"__GF-jIrBbVO","colab_type":"text"},"cell_type":"markdown","source":["## Compiling the model"]},{"metadata":{"id":"zvblzG-HBbVO","colab_type":"code","colab":{}},"cell_type":"code","source":["# Compile the model\n","batch_size = 64\n","\n","#opt_rms = keras.optimizers.rmsprop(lr=0.001, decay=1e-6) for momentum instead of adam use oppt_rms\n","model.compile(loss='categorical_crossentropy', \n","              optimizer='adam', \n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qsXvna3_BbVR","colab_type":"text"},"cell_type":"markdown","source":["## Training the algorithm\n","I would suggest more than 100 epochs"]},{"metadata":{"id":"N1vEA5P0BbVU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1097},"outputId":"f485bc26-99fa-41ce-ac97-566fcec12b1f","executionInfo":{"status":"ok","timestamp":1537281779684,"user_tz":-330,"elapsed":1088525,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["import time\n","start=time.time()\n","epochs = 30\n","model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                    steps_per_epoch=x_train.shape[0] // batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))\n","end=time.time()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","781/781 [==============================] - 39s 50ms/step - loss: 1.6591 - acc: 0.4141 - val_loss: 1.2649 - val_acc: 0.5483\n","Epoch 2/30\n","781/781 [==============================] - 37s 47ms/step - loss: 1.2767 - acc: 0.5395 - val_loss: 1.0373 - val_acc: 0.6326\n","Epoch 3/30\n","781/781 [==============================] - 36s 46ms/step - loss: 1.1574 - acc: 0.5868 - val_loss: 1.2556 - val_acc: 0.5691\n","Epoch 4/30\n","781/781 [==============================] - 36s 46ms/step - loss: 1.0886 - acc: 0.6155 - val_loss: 0.9664 - val_acc: 0.6549\n","Epoch 5/30\n","781/781 [==============================] - 35s 45ms/step - loss: 1.0403 - acc: 0.6326 - val_loss: 0.9906 - val_acc: 0.6512\n","Epoch 6/30\n","781/781 [==============================] - 36s 46ms/step - loss: 1.0022 - acc: 0.6467 - val_loss: 0.9021 - val_acc: 0.6798\n","Epoch 7/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.9793 - acc: 0.6548 - val_loss: 0.8991 - val_acc: 0.6761\n","Epoch 8/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.9490 - acc: 0.6687 - val_loss: 0.8285 - val_acc: 0.7101\n","Epoch 9/30\n","781/781 [==============================] - 36s 47ms/step - loss: 0.9330 - acc: 0.6720 - val_loss: 0.8038 - val_acc: 0.7135\n","Epoch 10/30\n","781/781 [==============================] - 37s 47ms/step - loss: 0.9141 - acc: 0.6798 - val_loss: 1.0762 - val_acc: 0.6463\n","Epoch 11/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.8903 - acc: 0.6887 - val_loss: 0.7747 - val_acc: 0.7225\n","Epoch 12/30\n","781/781 [==============================] - 35s 45ms/step - loss: 0.8855 - acc: 0.6900 - val_loss: 0.9039 - val_acc: 0.6743\n","Epoch 13/30\n","781/781 [==============================] - 36s 45ms/step - loss: 0.8687 - acc: 0.6961 - val_loss: 0.7537 - val_acc: 0.7370\n","Epoch 14/30\n","781/781 [==============================] - 37s 47ms/step - loss: 0.8482 - acc: 0.7008 - val_loss: 0.7900 - val_acc: 0.7276\n","Epoch 15/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.8403 - acc: 0.7068 - val_loss: 0.7036 - val_acc: 0.7543\n","Epoch 16/30\n","781/781 [==============================] - 36s 47ms/step - loss: 0.8247 - acc: 0.7127 - val_loss: 0.7066 - val_acc: 0.7533\n","Epoch 17/30\n","781/781 [==============================] - 37s 47ms/step - loss: 0.8148 - acc: 0.7154 - val_loss: 0.6931 - val_acc: 0.7607\n","Epoch 18/30\n","781/781 [==============================] - 37s 47ms/step - loss: 0.8056 - acc: 0.7190 - val_loss: 0.8575 - val_acc: 0.7058\n","Epoch 19/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.7975 - acc: 0.7218 - val_loss: 0.8414 - val_acc: 0.7047\n","Epoch 20/30\n","781/781 [==============================] - 35s 45ms/step - loss: 0.7862 - acc: 0.7238 - val_loss: 0.6699 - val_acc: 0.7662\n","Epoch 21/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.7757 - acc: 0.7304 - val_loss: 0.6466 - val_acc: 0.7781\n","Epoch 22/30\n","781/781 [==============================] - 36s 47ms/step - loss: 0.7757 - acc: 0.7285 - val_loss: 0.7021 - val_acc: 0.7513\n","Epoch 23/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.7622 - acc: 0.7327 - val_loss: 0.8084 - val_acc: 0.7139\n","Epoch 24/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.7659 - acc: 0.7314 - val_loss: 0.8044 - val_acc: 0.7184\n","Epoch 25/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.7583 - acc: 0.7383 - val_loss: 0.6629 - val_acc: 0.7706\n","Epoch 26/30\n","781/781 [==============================] - 38s 48ms/step - loss: 0.7442 - acc: 0.7381 - val_loss: 0.6809 - val_acc: 0.7642\n","Epoch 27/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.7396 - acc: 0.7421 - val_loss: 0.6496 - val_acc: 0.7719\n","Epoch 28/30\n","781/781 [==============================] - 35s 45ms/step - loss: 0.7361 - acc: 0.7419 - val_loss: 0.6426 - val_acc: 0.7744\n","Epoch 29/30\n","781/781 [==============================] - 36s 46ms/step - loss: 0.7311 - acc: 0.7457 - val_loss: 0.6988 - val_acc: 0.7624\n","Epoch 30/30\n","781/781 [==============================] - 37s 48ms/step - loss: 0.7302 - acc: 0.7469 - val_loss: 0.6904 - val_acc: 0.7577\n"],"name":"stdout"}]},{"metadata":{"id":"za4ciObTBbVb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"63053900-9636-454f-efc1-98da0baac551","executionInfo":{"status":"ok","timestamp":1537282678190,"user_tz":-330,"elapsed":1329,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["end-start"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1087.4245779514313"]},"metadata":{"tags":[]},"execution_count":11}]},{"metadata":{"id":"B4dYPkfnBbVe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}