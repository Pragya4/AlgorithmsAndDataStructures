{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [default]","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"},"colab":{"name":"DataProcessing_nptelData.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"pxH9XCHMa6qI","executionInfo":{"status":"ok","timestamp":1601802024554,"user_tz":-330,"elapsed":3253,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJHTlWfycyJm80lTQA74CTdS6PXP9L1Naymb6i=s64","userId":"06230978283826063243"}}},"source":["import pandas as pd\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Vulz_mHbKMe"},"source":[""]},{"cell_type":"code","metadata":{"id":"TBJsF2BVa6qM","executionInfo":{"status":"error","timestamp":1601761553372,"user_tz":-330,"elapsed":6708,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJHTlWfycyJm80lTQA74CTdS6PXP9L1Naymb6i=s64","userId":"06230978283826063243"}},"outputId":"794d03fd-b724-4d35-a6f6-7b474c161e39","colab":{"base_uri":"https://localhost:8080/","height":375}},"source":["df1=pd.read_csv('nptel_final.csv')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IOError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-2-96c830293e25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nptel_final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mIOError\u001b[0m: [Errno 2] File nptel_final.csv does not exist: 'nptel_final.csv'"]}]},{"cell_type":"code","metadata":{"id":"gm8QMQ_Ea6qR"},"source":["from nltk.corpus import stopwords\n","#converting stop words to ascii to prevent unicode error and adding some new words that could intuitively be stop words here\n","vtt_content=df1.vtt_Content.tolist()\n","stop_words_ascii=[]\n","for each in stopwords.words('english'):\n","    stop_words_ascii.append(each.encode('ascii','ignore'))\n","\n","stop_words_ascii.extend(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n","stop_words_ascii.extend(['ah','uh','the','and','so','is','ok','um','ok','ha','language:','en'])\n","\n","#print(stop_words_ascii)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fLxQP6SHa6qU"},"source":["#STEMMING"]},{"cell_type":"code","metadata":{"id":"l7gKfTsaa6qU"},"source":["import nltk\n","import string\n","import re\n","\n","porter_stemmer = nltk.stem.porter.PorterStemmer()\n","\n","def porter_tokenizer(text, stemmer=porter_stemmer):\n","    tokens = nltk.wordpunct_tokenize(text)\n","    stems = [porter_stemmer.stem(t) for t in tokens]\n","    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n","    return no_punct"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nd3cVQ8a6qX"},"source":["stemmed_vtt_content=[] \n","#stemming each word for each row of vtt_description of videos\n","for eachcontent in vtt_content:\n","    eachcontent=eachcontent.lower().decode('utf-8')\n","    #print(type(eachcontent))\n","    tokenized=porter_tokenizer(eachcontent)\n","    preprocessed=' '.join(tokenized)\n","    preprocessed=preprocessed.encode('ascii','ignore')\n","    #print(preprocessed)\n","    stemmed_vtt_content.append(preprocessed)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEwZt3qAa6qa"},"source":["#removing stop words from stemmed vtt data\n","filtered_words_list=[]\n","i=0\n","for i in range(len(stemmed_vtt_content)):\n","    filtered_words=[]\n","    for item in stemmed_vtt_content[i].split(\" \"):\n","        if item not in stop_words_ascii:\n","            filtered_words.append(item)         \n","    #print(filtered_words)\n","    str1=\" \".join(filtered_words)\n","    #print str1\n","    #print(\"\\n NEXT DOCUMENT______________________________________________________________________\",i)\n","    filtered_words_list.append(str1)\n","#print(\"FINAL_____________________________________________________________________\",filtered_words_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"itigv8cha6qc"},"source":["#checking the most often occuring words,utility function only for analysis \n","import collections\n","for i in range(len(filtered_words_list)):\n","    #for list_1 in filtered_words_list[i].split(\" \"):\n","    counter = collections.Counter(filtered_words_list[i].split(\" \"))\n","    #print(counter.most_common())\n","    #print(\"______________________________________________________________________________________________\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JRA6n3X_a6qg"},"source":["#Applying TFIDF"]},{"cell_type":"code","metadata":{"id":"QxUSZ2Iza6qh"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 0, stop_words = stop_words_ascii)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOkhJi6Va6qk"},"source":["tfidf_matrix =  tf.fit_transform(filtered_words_list)\n","feature_names = tf.get_feature_names() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yTnJRAWa6qm","outputId":"44a86ca5-f285-4daa-b84b-bb4ce053fa3a"},"source":["len(feature_names)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14682"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"0VfJloZAa6qq","outputId":"8936c2af-75b1-41da-b071-f08cd633f181"},"source":["tfidf_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<902x14682 sparse matrix of type '<type 'numpy.float64'>'\n","\twith 392333 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"AvpcQFSha6qs"},"source":["from pandas import DataFrame\n","doc_id=0\n","#showing 50 most important occuring words,as processed by tfidf\n","tfidf_words_list=[]\n","dense=tfidf_matrix.todense()\n","for i in range(len(filtered_words_list)):\n","    top_n=[]\n","    perVid_Transcript=dense[i].tolist()[0]\n","    phrase_scores= [pair for pair in zip(range(0, len(perVid_Transcript)), perVid_Transcript) if pair[1] > 0]\n","    sorted_phrase_scores=sorted(phrase_scores, key=lambda t: t[1] * -1)\n","    for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores][:50]:\n","        top_n.append(phrase.encode('ascii','ignore'))\n","        #print('{0} {1: <50} {2}'.format(i,phrase, score))\n","    tfidf_words_list.append(top_n)\n","    #print('\\n Document {0} :{1}'.format(i,top_n))\n","#print('FINAL list of 50 words of all docs in one list',tfidf_words_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjpQVhjra6qv"},"source":["#adding a new row in our dataframe of top tags with each vtt\n","df1['TFIDF_top_words']=tfidf_words_list\n","df1\n","df1.to_csv('nptel_tfidf_labelled.csv')\n","\n","#print df1['TFIDF_top_words']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoH_TI1na6qy"},"source":[""],"execution_count":null,"outputs":[]}]}