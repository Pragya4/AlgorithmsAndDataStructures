{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cifar-10-keras-2NoBatchNormalization+adam+relu.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"nVCHfI7wMYqZ","colab_type":"text"},"cell_type":"markdown","source":["# Handle the CIFAR-10 dataset"]},{"metadata":{"id":"6Cw2NHwLMYqb","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import pickle\n","import numpy as np\n","from os import listdir\n","from os.path import isfile, join\n","import os\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WcR0NV89MYqe","colab_type":"text"},"cell_type":"markdown","source":["## Create a function that will extract the data from the dataset\n","You can find the dataset here: https://www.cs.utoronto.ca/~kriz/cifar-10-python.tar.gz\n","\n","When you download the dataset you will get a bunch of pickled files <br>\n","Inside `cifar-10-batches-py`folder you will find <br>\n","1. data_batch_x files <br>\n","2. test_batch \n","\n","The data_batch_x files are the batched training data splited in x parts (in our case in 5 parts of 10000 samples each). The test_batch file contains 10000 samples for testing.\n","\n","### The stracture\n","All files has the same structure: <br>\n","1. data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n","2. labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n","\n","There is also another file called batches.meta where it describes the classes"]},{"metadata":{"id":"f5BQAUlCMYqf","colab_type":"code","colab":{}},"cell_type":"code","source":["# Function to unpickle the dataset\n","def unpickle_all_data(directory):\n","    \n","    # Initialize the variables\n","    train = dict()\n","    test = dict()\n","    train_x = []\n","    train_y = []\n","    test_x = []\n","    test_y = []\n","    \n","    # Iterate through all files that we want, train and test\n","    # Train is separated into batches\n","    for filename in listdir(directory):\n","        if isfile(join(directory, filename)):\n","            \n","            # The train data\n","            if 'data_batch' in filename:\n","                print('Handing file: %s' % filename)\n","                \n","                # Opent the file\n","                with open(directory + '/' + filename, 'rb') as fo:\n","                    data = pickle.load(fo)\n","\n","                if 'data' not in train:\n","                    train['data'] = data[b'data']\n","                    train['labels'] = np.array(data[b'labels'])\n","                else:\n","                    train['data'] = np.concatenate((train['data'], data[b'data']))\n","                    train['labels'] = np.concatenate((train['labels'], data[b'labels']))\n","            # The test data\n","            elif 'test_batch' in filename:\n","                print('Handing file: %s' % filename)\n","                \n","                # Open the file\n","                with open(directory + '/' + filename, 'rb') as fo:\n","                    data = pickle.load(fo)\n","                \n","                test['data'] = data[b'data']\n","                test['labels'] = data[b'labels']\n","    \n","    # Manipulate the data to the propper format\n","    for image in train['data']:\n","        train_x.append(np.transpose(np.reshape(image,(3, 32,32)), (1,2,0)))\n","    train_y = [label for label in train['labels']]\n","    \n","    for image in test['data']:\n","        test_x.append(np.transpose(np.reshape(image,(3, 32,32)), (1,2,0)))\n","    test_y = [label for label in test['labels']]\n","    \n","    # Transform the data to np array format\n","    train_x = np.array(train_x)\n","    train_y = np.array(train_y)\n","    test_x = np.array(test_x)\n","    test_y = np.array(test_y)\n","    \n","    return (train_x, train_y), (test_x, test_y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DQZ7dgj5MYqh","colab_type":"text"},"cell_type":"markdown","source":["## Using the function\n","\n","Returns four variables\n","1. x_train := A 3D numpy array with the images in a format (32, 32, 3) a 32x32 image with 3 dimmesions for colors RGB\n","2. y_train := A 1D numpy with the numbered labels for each sample stom x_train\n","3. x_test := same as x_train\n","4. y_test := same as y_train"]},{"metadata":{"id":"lXCiG9vIMYqj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2b787e31-a01b-43fe-dbed-2ae3c70fd504","executionInfo":{"status":"ok","timestamp":1537282956351,"user_tz":-330,"elapsed":2038,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["#/home/pragya/Desktop/IIITBangalore/Semester3/AVR/Tensorflow_AVR/sagemaker-deep-learning-master/cifar-10-keras-mxnet/sagemaker\n","from keras.datasets import cifar10 \n","#(x_train, y_train), (x_test, y_test) = unpickle_all_data(os.getcwd() + '/cifar-10-batches-py/')\n","(x_train, y_train), (x_test, y_test)=cifar10.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"eGLhhv1SMYqn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"8459291a-baed-4622-83a5-30a5afd2be3d","executionInfo":{"status":"ok","timestamp":1537282959681,"user_tz":-330,"elapsed":1246,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3)\n","(50000, 1)\n","(10000, 32, 32, 3)\n","(10000, 1)\n"],"name":"stdout"}]},{"metadata":{"id":"SWKNPTwLMYqr","colab_type":"text"},"cell_type":"markdown","source":["## Preprocess the data\n","1. The data must be in a float32 represenation\n","\n","No 2. The data should be normalized, here I chose [0,1] normalization-https://kth.diva-portal.org/smash/get/diva2:955562/FULLTEXT01.pdf\n","https://stats.stackexchange.com/questions/285133/is-batch-normalization-useful-outside-of-convolutional-networks\n","3. The labels are one-hot encoded"]},{"metadata":{"id":"yLLbeeXqMYqr","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.utils import np_utils\n","\n","# Transofrm them to a float32 type\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# Normalize the input \n","x_train /= 255\n","x_test /= 255\n","\n","# One-hot Encoding\n","num_classes = 10\n","y_train = np_utils.to_categorical(y_train, num_classes)\n","y_test = np_utils.to_categorical(y_test, num_classes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1ZMTGhFtMYqv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"d148419b-3ae4-4ca4-eb89-ce911cdb9628","executionInfo":{"status":"ok","timestamp":1537282966326,"user_tz":-330,"elapsed":1084,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(50000, 32, 32, 3)\n","(50000, 10)\n","(10000, 32, 32, 3)\n","(10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"riI_bSY9MYq0","colab_type":"text"},"cell_type":"markdown","source":["## Building the model\n","1. The input data for the model is each image sample 32x32x3\n","2. Convolve the image into 3x3 squares with padding same (https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t) and Pooling with 2x2. Add a dropout of 0.2 to avoid early overfitting\n","3. Same situation for the next two layers for 64 input size, and 128 input size\n","4. Finally, add a Dense(fully connected) layer for the output\n","\n","You can find more information about the model here: https://appliedmachinelearning.wordpress.com/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n","\n","You can try different layouts to improve the accuracy"]},{"metadata":{"id":"dNALIldMMYq1","colab_type":"code","colab":{}},"cell_type":"code","source":["import keras\n","from keras.models import Sequential\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.datasets import cifar10\n","from keras import regularizers\n","from keras.callbacks import LearningRateScheduler\n","import numpy as np\n","\n","# Create the model\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same',\n","                 input_shape=x_train.shape[1:]))\n","model.add(Activation('relu'))\n","model.add(Conv2D(32, (3, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","#model.add(Conv2D(64, (3, 3), padding='same'))\n","#model.add(Activation('relu'))\n","#model.add(Conv2D(64, (3, 3)))\n","#model.add(Activation('relu'))\n","#model.add(MaxPooling2D(pool_size=(2, 2)))\n","#model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(512))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(256))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","#model.add(Dense(num_classes, activation='softmax'))\n","\n","model.add(Dense(num_classes))\n","model.add(Activation('softmax'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NyeDI5-sMYq4","colab_type":"text"},"cell_type":"markdown","source":["## Augment the data\n","From one sample, extract multiple versions\n","\n","For example, rotate the image, shift, flip etc"]},{"metadata":{"id":"vbleMSPXMYq5","colab_type":"code","colab":{}},"cell_type":"code","source":["# data augmentation\n","datagen = ImageDataGenerator(\n","    featurewise_center=False,  # set input mean to 0 over the dataset\n","    samplewise_center=False,  # set each sample mean to 0\n","    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","    samplewise_std_normalization=False,  # divide each input by its std\n","    zca_whitening=False,  # apply ZCA whitening\n","    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n","    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","    horizontal_flip=True,  # randomly flip images\n","    vertical_flip=False)  # randomly flip images\n","\n","# Compute quantities required for feature-wise normalization\n","# (std, mean, and principal components if ZCA whitening is applied).\n","datagen.fit(x_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QqhhaxysMYq8","colab_type":"text"},"cell_type":"markdown","source":["## Compiling the model"]},{"metadata":{"id":"jwlbuhtcMYq9","colab_type":"code","colab":{}},"cell_type":"code","source":["# Compile the model\n","batch_size = 64\n","\n","opt_rms = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', \n","              optimizer='adam', \n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7P2j2EXsMYq_","colab_type":"text"},"cell_type":"markdown","source":["## Training the algorithm\n","I would suggest more than 100 epochs"]},{"metadata":{"id":"sHWajJJUMYrA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1097},"outputId":"96b2e43a-2907-402a-c428-592af6b778ac","executionInfo":{"status":"ok","timestamp":1537284071027,"user_tz":-330,"elapsed":986761,"user":{"displayName":"Pragya Gupta","photoUrl":"//lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s50-c-k-no/photo.jpg","userId":"105780252399906013527"}}},"cell_type":"code","source":["import time\n","start=time.time()\n","epochs = 30\n","model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                    steps_per_epoch=x_train.shape[0] // batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))\n","end=time.time()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","781/781 [==============================] - 32s 41ms/step - loss: 1.5657 - acc: 0.4307 - val_loss: 1.2451 - val_acc: 0.5528\n","Epoch 2/30\n","781/781 [==============================] - 33s 42ms/step - loss: 1.3269 - acc: 0.5265 - val_loss: 1.0784 - val_acc: 0.6155\n","Epoch 3/30\n","781/781 [==============================] - 33s 42ms/step - loss: 1.2250 - acc: 0.5673 - val_loss: 1.0101 - val_acc: 0.6480\n","Epoch 4/30\n","781/781 [==============================] - 33s 42ms/step - loss: 1.1654 - acc: 0.5878 - val_loss: 0.9690 - val_acc: 0.6573\n","Epoch 5/30\n","781/781 [==============================] - 32s 41ms/step - loss: 1.1349 - acc: 0.6039 - val_loss: 0.9829 - val_acc: 0.6573\n","Epoch 6/30\n","781/781 [==============================] - 32s 41ms/step - loss: 1.0985 - acc: 0.6127 - val_loss: 0.8936 - val_acc: 0.6825\n","Epoch 7/30\n","781/781 [==============================] - 33s 42ms/step - loss: 1.0783 - acc: 0.6229 - val_loss: 0.8726 - val_acc: 0.6952\n","Epoch 8/30\n","781/781 [==============================] - 32s 41ms/step - loss: 1.0509 - acc: 0.6332 - val_loss: 0.9387 - val_acc: 0.6652\n","Epoch 9/30\n","781/781 [==============================] - 33s 42ms/step - loss: 1.0335 - acc: 0.6365 - val_loss: 0.8819 - val_acc: 0.6891\n","Epoch 10/30\n","781/781 [==============================] - 32s 41ms/step - loss: 1.0242 - acc: 0.6434 - val_loss: 0.8309 - val_acc: 0.7045\n","Epoch 11/30\n","781/781 [==============================] - 33s 42ms/step - loss: 1.0089 - acc: 0.6467 - val_loss: 0.8500 - val_acc: 0.6992\n","Epoch 12/30\n","781/781 [==============================] - 32s 41ms/step - loss: 0.9975 - acc: 0.6537 - val_loss: 0.9029 - val_acc: 0.6822\n","Epoch 13/30\n","781/781 [==============================] - 32s 41ms/step - loss: 0.9784 - acc: 0.6625 - val_loss: 0.8647 - val_acc: 0.6929\n","Epoch 14/30\n","781/781 [==============================] - 33s 42ms/step - loss: 0.9769 - acc: 0.6597 - val_loss: 0.8327 - val_acc: 0.7070\n","Epoch 15/30\n","781/781 [==============================] - 34s 43ms/step - loss: 0.9711 - acc: 0.6618 - val_loss: 0.8023 - val_acc: 0.7180\n","Epoch 16/30\n","781/781 [==============================] - 33s 42ms/step - loss: 0.9584 - acc: 0.6666 - val_loss: 0.7936 - val_acc: 0.7224\n","Epoch 17/30\n","781/781 [==============================] - 33s 42ms/step - loss: 0.9505 - acc: 0.6676 - val_loss: 0.8146 - val_acc: 0.7117\n","Epoch 18/30\n","781/781 [==============================] - 34s 43ms/step - loss: 0.9467 - acc: 0.6702 - val_loss: 0.8409 - val_acc: 0.7037\n","Epoch 19/30\n","781/781 [==============================] - 33s 43ms/step - loss: 0.9371 - acc: 0.6764 - val_loss: 0.7857 - val_acc: 0.7291\n","Epoch 20/30\n","781/781 [==============================] - 34s 43ms/step - loss: 0.9288 - acc: 0.6766 - val_loss: 0.7844 - val_acc: 0.7296\n","Epoch 21/30\n","781/781 [==============================] - 32s 41ms/step - loss: 0.9275 - acc: 0.6762 - val_loss: 0.7508 - val_acc: 0.7373\n","Epoch 22/30\n","781/781 [==============================] - 34s 43ms/step - loss: 0.9221 - acc: 0.6831 - val_loss: 0.7480 - val_acc: 0.7373\n","Epoch 23/30\n","781/781 [==============================] - 32s 41ms/step - loss: 0.9191 - acc: 0.6808 - val_loss: 0.8103 - val_acc: 0.7118\n","Epoch 24/30\n","781/781 [==============================] - 33s 42ms/step - loss: 0.9194 - acc: 0.6805 - val_loss: 0.7702 - val_acc: 0.7318\n","Epoch 25/30\n","781/781 [==============================] - 34s 43ms/step - loss: 0.9042 - acc: 0.6856 - val_loss: 0.7464 - val_acc: 0.7390\n","Epoch 26/30\n","781/781 [==============================] - 33s 43ms/step - loss: 0.9041 - acc: 0.6880 - val_loss: 0.7900 - val_acc: 0.7208\n","Epoch 27/30\n","781/781 [==============================] - 34s 43ms/step - loss: 0.9022 - acc: 0.6894 - val_loss: 0.7669 - val_acc: 0.7349\n","Epoch 28/30\n","781/781 [==============================] - 33s 42ms/step - loss: 0.8974 - acc: 0.6897 - val_loss: 0.7180 - val_acc: 0.7495\n","Epoch 29/30\n","781/781 [==============================] - 33s 42ms/step - loss: 0.8893 - acc: 0.6921 - val_loss: 0.7459 - val_acc: 0.7399\n","Epoch 30/30\n","781/781 [==============================] - 32s 41ms/step - loss: 0.8831 - acc: 0.6948 - val_loss: 0.7724 - val_acc: 0.7278\n"],"name":"stdout"}]},{"metadata":{"id":"enRTjhSFMYrE","colab_type":"code","colab":{},"outputId":"d398fbd6-49dc-4f88-e7a2-1c9369551b17"},"cell_type":"code","source":["end-start"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1960.4112691879272"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"u7ZOkZn5MYrJ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"IbG6XGXrMYrL","colab_type":"code","colab":{},"outputId":"79286694-084f-4f37-a2bc-a08d3b349627"},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy on test data is: 73.06\n"],"name":"stdout"}]},{"metadata":{"id":"FUqzPzmfMYrO","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}