{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"kMLPQa-0WGtf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"9216179d-48a0-494c-969f-2953c2650374","executionInfo":{"status":"ok","timestamp":1542972811290,"user_tz":-330,"elapsed":5215,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["followMLdata.xlsx  sample_data\n"],"name":"stdout"}]},{"metadata":{"id":"HNopqAAXWJNn","colab_type":"code","colab":{}},"cell_type":"code","source":["#import all the necessary packages.\n","\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import warnings\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","import math\n","import time\n","import re\n","import os\n","import seaborn as sns\n","from collections import Counter\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity  \n","from sklearn.metrics import pairwise_distances\n","from matplotlib import gridspec\n","from scipy.sparse import hstack\n","#import plotly\n","#import plotly.figure_factory as ff\n","#from plotly.graph_objs import Scatter, Layout\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XbEIu9QMWi34","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"cb97cfdc-b8e1-490f-d1e1-467fd1c9ed5a","executionInfo":{"status":"ok","timestamp":1542973018952,"user_tz":-330,"elapsed":876,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["data = pd.read_csv('followMLdata.csv')\n","data['question'].head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                            Are you a self-motivator?\n","1    What matters to you more - job satisfaction or...\n","2    What matters to you more - job satisfaction or...\n","3    Have you worked with someone unprofessional, h...\n","4    Have you worked with someone unprofessional, h...\n","Name: question, dtype: object"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"0xX0UJgtW8-D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"e52894d4-dc3e-47c9-d4f7-1154fa12f452","executionInfo":{"status":"ok","timestamp":1542973028276,"user_tz":-330,"elapsed":1033,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["data.columns"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Unnamed: 0', 'question', 'answer', 'followUp'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"2Y-GjDIHW_NJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"443bbdbc-fbb1-40b8-ad33-32ca6d994ce5","executionInfo":{"status":"ok","timestamp":1542973037565,"user_tz":-330,"elapsed":960,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["data.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1086, 4)"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"uzs7OJDPXBfj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"d94fc713-3828-4028-84b8-b1634ebc566b","executionInfo":{"status":"ok","timestamp":1542973044029,"user_tz":-330,"elapsed":911,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["data['answer'].describe()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count                                                  1086\n","unique                                                  465\n","top       Once we had to organize an event in our colleg...\n","freq                                                     10\n","Name: answer, dtype: object"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"AXdhztXUXDFE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1612},"outputId":"ccb9d885-7bbb-4eb9-c7db-082f50dc2685","executionInfo":{"status":"error","timestamp":1542973061136,"user_tz":-330,"elapsed":983,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["# we use the list of stop words that are downloaded from nltk lib.\n","stop_words = set(stopwords.words('english'))\n","print ('list of stop words:', stop_words)\n","\n","def nlp_preprocessing(total_text, index, column):\n","    if type(total_text) is not int:\n","        string = \"\"\n","        for words in total_text.split():\n","            # remove the special chars in review like '\"#$@!%^&*()_+-~?>< etc.\n","            word = (\"\".join(e for e in words if e.isalnum()))\n","            # Conver all letters to lower-case\n","            word = word.lower()\n","            # stop-word removal\n","            if not word in stop_words:\n","                string += word + \" \"\n","        data[column][index] = string"],"execution_count":8,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-94197d5905b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'list of stop words:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnlp_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"]}]},{"metadata":{"id":"e-Q-pMDXXHPi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"f65cdc39-6856-434f-9a56-bbeb02c16ab3","executionInfo":{"status":"ok","timestamp":1542973158827,"user_tz":-330,"elapsed":969,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"kvHJsbRuXfFy","colab_type":"code","colab":{}},"cell_type":"code","source":["qa = data[['question','answer']]\n","target = data[['followUp']]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BC-ou9gsXyui","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","import gensim\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","import pickle"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sEJu7kTpX3T-","colab_type":"code","colab":{}},"cell_type":"code","source":["with open('word2vec_model', 'rb') as handle:\n","    model = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V_51CnQwaPCu","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_word_vec(sentence, doc_id, m_name):\n","    # sentence : title of the apparel\n","    # doc_id: document id in our corpus\n","    # m_name: model information it will take two values\n","        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n","        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n","    vec = []\n","    for i in sentence.split():\n","        if i in vocab:\n","            if m_name == 'weighted' and i in  idf_title_vectorizer.vocabulary_:\n","                vec.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[i]] * model[i])\n","            elif m_name == 'avg':\n","                vec.append(model[i])\n","        else:\n","            # if the word in our courpus is not there in the google word2vec corpus, we are just ignoring it\n","            vec.append(np.zeros(shape=(300,)))\n","    # we will return a numpy array of shape (#number of words in title * 300 ) 300 = len(w2v_model[word])\n","    # each row represents the word2vec representation of each word (weighted/avg) in given sentance \n","    return  np.array(vec)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8EBae7uqaY9e","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_distance(vec1, vec2):\n","    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n","    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n","    \n","    final_dist = []\n","    # for each vector in vec1 we caluclate the distance(euclidean) to all vectors in vec2\n","    for i in vec1:\n","        dist = []\n","        for j in vec2:\n","            # np.linalg.norm(i-j) will result the euclidean distance between vectors i, j\n","            dist.append(np.linalg.norm(i-j))\n","        final_dist.append(np.array(dist))\n","    # final_dist = np.array(#number of words in title1 * #number of words in title2)\n","    # final_dist[i,j] = euclidean distance between vectors i, j\n","    return np.array(final_dist)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7lxd7j7ojGzL","colab_type":"code","colab":{}},"cell_type":"code","source":["vocab = model.keys()\n","# this function will add the vectors of each word and returns the avg vector of given sentance\n","def build_avg_vec(sentence, num_features, doc_id, m_name):\n","    # sentace: its title of the apparel\n","    # num_features: the lenght of word2vec vector, its values = 300\n","    # m_name: model information it will take two values\n","        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n","        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n","\n","    featureVec = np.zeros((num_features,), dtype=\"float32\")\n","    # we will intialize a vector of size 300 with all zeros\n","    # we add each word2vec(wordi) to this fetureVec\n","    nwords = 0\n","    \n","    for word in sentence.split():\n","        nwords += 1\n","        if word in vocab:\n","            if m_name == 'weighted' and word in  idf_title_vectorizer.vocabulary_:\n","                featureVec = np.add(featureVec, idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[word]] * model[word])\n","            elif m_name == 'avg':\n","                featureVec = np.add(featureVec, model[word])\n","    if(nwords>0):\n","        featureVec = np.divide(featureVec, nwords)\n","    # returns the avg vector of given sentance, its of shape (1, 300)\n","    return featureVec"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OR6RJhhZr6fl","colab_type":"code","colab":{}},"cell_type":"code","source":["list1=[]\n","for i in range(len(qa)):\n","  list1.append((qa['question'][i]+qa['answer'][i]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l9ycSJ7TnT_8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b5aa44c6-e8b3-4e5a-b9db-0f04095e627a","executionInfo":{"status":"ok","timestamp":1542978703926,"user_tz":-330,"elapsed":998,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":["doc_id = 0\n","w2v_title = []\n","# for every title we build a avg vector representation\n","for i in list1:\n","    #print(i , build_avg_vec(i, 300, doc_id,'avg'))\n","    w2v_title.append(build_avg_vec(i, 300, doc_id,'avg'))\n","    doc_id += 1\n","\n","# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc \n","\n","w2v_title = np.array(w2v_title) \n","print(w2v_title.shape)"],"execution_count":104,"outputs":[{"output_type":"stream","text":["(1086, 300)\n"],"name":"stdout"}]},{"metadata":{"id":"Bh0fKax1nnFe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"6yjDtNVLo-Ch","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"37fad1b1-3830-4e22-aeb2-c593dad4f315","executionInfo":{"status":"ok","timestamp":1542978468146,"user_tz":-330,"elapsed":1008,"user":{"displayName":"Pragya Gupta","photoUrl":"https://lh4.googleusercontent.com/-NRqKxWdzLhk/AAAAAAAAAAI/AAAAAAAAAFQ/hPyHB_AiaE0/s64/photo.jpg","userId":"06230978283826063243"}}},"cell_type":"code","source":[""],"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Are you a self-motivator?Absolutely. For me, internal motivation works far more than external motivation ever could. Yes, at first, it may seem like I want some sort of external motivation, but the very end, my heart goes into the work assigned only when my own self pushes me to do it.'"]},"metadata":{"tags":[]},"execution_count":98}]},{"metadata":{"id":"_Mhtd_xqp6Sl","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}